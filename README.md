# Real-time Neural Renderer
![Banner Logo](https://polybiustech.github.io/NeuralRenderer/static/images/teaser.png)

Latent diffusion models have high speed image synthesis capabilities that can used for manipulating existing images and synthetic 3D geometry with ease, having the potential to greatly increase the perceived realism with minimal effort.

## The Idea
Neural rendering is a concept that could bring great benefits to the field of computer graphics. By removing the dependency upon high resolution textures and complicated global illumination methods to generate photorealistic images, we can substantially increase the capabilities of prodecural generation algorithms, since a much lower level of detail is necessary to produce quality results. In place of these, we can use generative AI models to fill in the gaps so that the frames retain a high level of intricacy, and allow us to switch themes, styles, and settings instantaneously. In this project, we will explore the usage of Latent Consistency Models as a method of realtime neural rendering, in conjunction with StreamDiffusion, ControlNet and TemporalConsistency models.

## Demo
[![Neural Renderer Demo](http://img.youtube.com/vi/SHTunCmTqFg/0.jpg)](http://www.youtube.com/watch?v=SHTunCmTqFg "Project Morpheus - v0.3 Demo")

## Implementation and Results
This project repository is structured around the fantastic [StreamDiffusion](https://github.com/cumulo-autumn/StreamDiffusion). StreamDiffusion builds upon the existing work of Latent Consistency Models (LCM), which explore a method of distilling Stable Diffusion generative image models allowing them to produce coherent results in a fraction of the time (~50 steps -> ~5 steps). StreamDiffusion improves upon LCMs by introducing a batch diffusion pipeline, trading memory for speed. In addition, the authors present a novel residual classifier-free guidance (RCFG) algorithm that signifcantly outperforms the existing classifier-free guidance by up to 2.05x. By utilizing StreamDiffusion and finetuning the hyperparameters for quality and speed, we were able to acheive coherent visual results at an average frame rate of 11.9 fps on an RTX 3060, with minimal latency. Additionally, we utilize the TensorRT framework and model quantization methods (via [Microsoft Olive](https://github.com/microsoft/Olive)) that allow us to reach speeds up to 19 fps! While this is still below our goal of 24 fps (standard for general television quality), and is far from the speed that real-time graphics have come to enjoy on modern computers, it certainly is a step in the right direction. In terms of visual quality, however, this iteration was still underwhelming. Whilst the results where coherent, they were by no means practical or a great improvement upon standard graphic pipelines. To remedy this, we utilized the Depth and [TemporalNet](https://huggingface.co/CiaraRowles/TemporalNet) [ControlNets](https://github.com/lllyasviel/ControlNet). ControlNet is an additional non-text conditioning method for diffusion models that works by locking the parameters of the main model and training injectable Unet blocks that can be used modularly in the future. By using a depth controlnet, we gain a finer form of control over the outputs. But of arguably far greater importance is the TemporalNet. This model is trained for coherence between frames, which allows us to greatly reduce the "skipping" and "jittering" that can been seen in previous iterations of the pipeline.

## Conclusion
In conclusion, real-time neural rendering represents a promising frontier in computer graphics, offering the potential to revolutionize image synthesis and enhance realism with minimal computational effort. While significant progress has been made in improving speed and coherence, further advancements are needed to achieve television-quality frame rates and practical visual enhancements. Integrating additional techniques like TemporalNet and ControlNets shows promise in addressing these challenges, paving the way for more immersive and efficient rendering techniques in the future.
